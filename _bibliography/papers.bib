---
---

@misc{jiang2023training,
  title         = {Training neural operators to preserve invariant measures of chaotic attractors},
  author        = {Ruoxi Jiang and Peter Y. Lu and Elena Orlova and Rebecca Willett},
  year          = {2023},
  eprint        = {2306.01187},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  arxiv         = {2306.01187},
  pdf           = {https://arxiv.org/pdf/2306.01187.pdf},
  abstract      = {Chaotic systems make long-horizon forecasts difficult because small perturbations in initial conditions cause trajectories to diverge at an exponential rate. In this setting, neural operators trained to minimize squared error losses, while capable of accurate short-term forecasts, often fail to reproduce statistical or structural properties of the dynamics over longer time horizons and can yield degenerate results. In this paper, we propose an alternative framework designed to preserve invariant measures of chaotic attractors that characterize the time-invariant statistical properties of the dynamics. Specifically, in the multi-environment setting (where each sample trajectory is governed by slightly different dynamics), we consider two novel approaches to training with noisy data. First, we propose a loss based on the optimal transport distance between the observed dynamics and the neural operator outputs. This approach requires expert knowledge of the underlying physics to determine what statistical features should be included in the optimal transport loss. Second, we show that a contrastive learning framework, which does not require any specialized prior knowledge, can preserve statistical properties of the dynamics nearly as well as the optimal transport approach. On a variety of chaotic systems, our method is shown empirically to preserve invariant measures of chaotic attractors.},
  comment       = { --- Accepted at NeurIPS 2023},
  selected      = {true}
}

@misc{orlova2023deep,
  title         = {Deep Stochastic Mechanics},
  author        = {Elena Orlova and Aleksei Ustimenko and Ruoxi Jiang and Peter Y. Lu and Rebecca Willett},
  year          = {2023},
  eprint        = {2305.19685},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  arxiv         = {2305.19685},
  pdf           = {https://arxiv.org/pdf/2305.19685.pdf},
  abstract      = {This paper introduces a novel deep-learning-based approach for numerical simulation of a time-evolving Schrödinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational complexity with respect to the number of dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics.}
}

@article{10253952,
  author   = {Zhang, Michael and Kim, Samuel and Lu, Peter Y. and Soljačić, Marin},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  title    = {Deep Learning and Symbolic Regression for Discovering Parametric Equations},
  year     = {2023},
  volume   = {},
  number   = {},
  pages    = {1-13},
  doi      = {10.1109/TNNLS.2023.3297978},
  abstract = {Symbolic regression is a machine learning technique that can learn the equations governing data and thus has the potential to transform scientific discovery. However, symbolic regression is still limited in the complexity and dimensionality of the systems that it can analyze. Deep learning, on the other hand, has transformed machine learning in its ability to analyze extremely complex and high-dimensional datasets. We propose a neural network architecture to extend symbolic regression to parametric systems where some coefficient may vary, but the structure of the underlying governing equation remains constant. We demonstrate our method on various analytic expressions and partial differential equations (PDEs) with varying coefficients and show that it extrapolates well outside of the training domain. The proposed neural-network-based architecture can also be enhanced by integrating with other deep learning architectures such that it can analyze high-dimensional data while being trained end-to-end. To this end, we demonstrate the scalability of our architecture by incorporating a convolutional encoder to analyze 1-D images of varying spring systems.},
  url      = {https://ieeexplore.ieee.org/document/10253952},
  html     = {https://ieeexplore.ieee.org/document/10253952},
  pdf      = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10253952}
}

@article{Lu2023,
  author   = {Lu, Peter Y.
              and Dangovski, Rumen
              and Solja{\v{c}}i{\'{c}}, Marin},
  title    = {Discovering conservation laws using optimal transport and manifold learning},
  journal  = {Nature Communications},
  year     = {2023},
  month    = {Aug},
  day      = {07},
  volume   = {14},
  number   = {1},
  pages    = {4744},
  abstract = {Conservation laws are key theoretical and practical tools for understanding, characterizing, and modeling nonlinear dynamical systems. However, for many complex systems, the corresponding conserved quantities are difficult to identify, making it hard to analyze their dynamics and build stable predictive models. Current approaches for discovering conservation laws often depend on detailed dynamical information or rely on black box parametric deep learning methods. We instead reformulate this task as a manifold learning problem and propose a non-parametric approach for discovering conserved quantities. We test this new approach on a variety of physical systems and demonstrate that our method is able to both identify the number of conserved quantities and extract their values. Using tools from optimal transport theory and manifold learning, our proposed method provides a direct geometric approach to identifying conservation laws that is both robust and interpretable without requiring an explicit model of the system nor accurate time information.},
  issn     = {2041-1723},
  doi      = {10.1038/s41467-023-40325-7},
  url      = {https://doi.org/10.1038/s41467-023-40325-7},
  html     = {https://doi.org/10.1038/s41467-023-40325-7},
  pdf      = {https://www.nature.com/articles/s41467-023-40325-7.pdf},
  poster   = {conservation_laws_poster.pdf},
  code     = {https://github.com/peterparity/conservation-laws-manifold-learning},
  selected = {true}
}

@inproceedings{pmlr-v202-dugan23a,
  title     = {Q-Flow: Generative Modeling for Differential Equations of Open Quantum Dynamics with Normalizing Flows},
  author    = {Dugan, Owen M and Lu, Peter Y. and Dangovski, Rumen and Luo, Di and Soljačić, Marin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages     = {8879--8901},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume    = {202},
  series    = {Proceedings of Machine Learning Research},
  month     = {23--29 Jul},
  publisher = {PMLR},
  abstract  = {Studying the dynamics of open quantum systems can enable breakthroughs both in fundamental physics and applications to quantum engineering and quantum computation. Since the density matrix $\rho$, which is the fundamental description for the dynamics of such systems, is high-dimensional, customized deep generative neural networks have been instrumental in modeling $\rho$. However, the complex-valued nature and normalization constraints of $\rho$, as well as its complicated dynamics, prohibit a seamless connection between open quantum systems and the recent advances in deep generative modeling. Here we lift that limitation by utilizing a reformulation of open quantum system dynamics to a partial differential equation (PDE) for a corresponding probability distribution $Q$, the Husimi Q function. Thus, we model the Q function seamlessly with <em>off-the-shelf</em> deep generative models such as normalizing flows. Additionally, we develop novel methods for learning normalizing flow evolution governed by high-dimensional PDEs based on the Euler method and the application of the time-dependent variational principle. We name the resulting approach <em>Q-Flow</em> and demonstrate the scalability and efficiency of Q-Flow on open quantum system simulations, including the dissipative harmonic oscillator and the dissipative bosonic model. Q-Flow is superior to conventional PDE solvers and state-of-the-art physics-informed neural network solvers, especially in high-dimensional systems.},
  pdf       = {https://proceedings.mlr.press/v202/dugan23a/dugan23a.pdf},
  slides    = {https://icml.cc/media/icml-2023/Slides/23549.pdf},
  poster    = {https://icml.cc/media/PosterPDFs/ICML%202023/23549.png?t=1688703681.6640208},
  video     = {https://icml.cc/virtual/2023/poster/23549},
  url       = {https://proceedings.mlr.press/v202/dugan23a.html},
  html      = {https://proceedings.mlr.press/v202/dugan23a.html},
  selected  = {true}
}

@inproceedings{hernandez2022model,
  title     = {Model Stitching: Looking For Functional Similarity Between Representations},
  author    = {Adriano Hernandez and Rumen Dangovski and Peter Y. Lu and Marin Soljačić},
  booktitle = {SVRHM 2022 Workshop @ NeurIPS},
  year      = {2022},
  url       = {https://openreview.net/forum?id=7bHLCO5FQdB},
  html      = {https://openreview.net/forum?id=7bHLCO5FQdB},
  pdf       = {https://openreview.net/pdf?id=7bHLCO5FQdB},
  arxiv     = {2303.11277},
  abstract  = {Model stitching (Lenc & Vedaldi 2015) is a compelling methodology to compare different neural network representations, because it allows us to measure to what degree they may be interchanged. We expand on a previous work from Bansal, Nakkiran & Barak which used model stitching to compare representations of the same shapes learned by differently seeded and/or trained neural networks of the same architecture. Our contribution enables us to compare the representations learned by layers with different shapes from neural networks with different architectures. We subsequently reveal unexpected behavior of model stitching. Namely, we find that stitching, based on convolutions, for small ResNets, can reach high accuracy if those layers come later in the first (sender) network than in the second (receiver), even if those layers are far apart.}
}

@article{kim2022deep,
  title    = {Deep Learning for Bayesian Optimization of Scientific Problems with High-Dimensional Structure},
  author   = {Samuel Kim and Peter Y. Lu and Charlotte Loh and Jamie Smith and Jasper Snoek and Marin Soljačić},
  journal  = {Transactions of Machine Learning Research},
  year     = {2022},
  abstract = {Bayesian optimization (BO) is a popular paradigm for global optimization of expensive black-box functions, but there are many domains where the function is not completely a black-box. The data may have some known structure (e.g. symmetries) and/or the data generation process may be a composite process that yields useful intermediate or auxiliary information in addition to the value of the optimization objective. However, surrogate models traditionally employed in BO, such as Gaussian Processes (GPs), scale poorly with dataset size and do not easily accommodate known structure. Instead, we use Bayesian neural networks, a class of scalable and flexible surrogate models with inductive biases, to extend BO to complex, structured problems with high dimensionality. We demonstrate BO on a number of realistic problems in physics and chemistry, including topology optimization of photonic crystal materials using convolutional neural networks, and chemical property optimization of molecules using graph neural networks. On these complex tasks, we show that neural networks often outperform GPs as surrogate models for BO in terms of both sampling efficiency and computational cost.},
  url      = {https://openreview.net/forum?id=tPMQ6Je2rB},
  html     = {https://openreview.net/forum?id=tPMQ6Je2rB},
  pdf      = {https://openreview.net/pdf?id=tPMQ6Je2rB},
  code     = {https://github.com/samuelkim314/DeepBO},
  selected = {true}
}

@article{Lu2022,
  author   = {Lu, Peter Y.
              and Ari{\~{n}}o Bernad, Joan
              and Solja{\v{c}}i{\'{c}}, Marin},
  title    = {Discovering sparse interpretable dynamics from partial observations},
  journal  = {Communications Physics},
  year     = {2022},
  month    = {Aug},
  day      = {12},
  volume   = {5},
  number   = {1},
  pages    = {206},
  abstract = {Identifying the governing equations of a nonlinear dynamical system is key to both understanding the physical features of the system and constructing an accurate model of the dynamics that generalizes well beyond the available data. Achieving this kind of interpretable system identification is even more difficult for partially observed systems. We propose a machine learning framework for discovering the governing equations of a dynamical system using only partial observations, combining an encoder for state reconstruction with a sparse symbolic model. The entire architecture is trained end-to-end by matching the higher-order symbolic time derivatives of the sparse symbolic model with finite difference estimates from the data. Our tests show that this method can successfully reconstruct the full system state and identify the equations of motion governing the underlying dynamics for a variety of ordinary differential equation (ODE) and partial differential equation (PDE) systems.},
  issn     = {2399-3650},
  doi      = {10.1038/s42005-022-00987-z},
  url      = {https://doi.org/10.1038/s42005-022-00987-z},
  html     = {https://doi.org/10.1038/s42005-022-00987-z},
  pdf      = {https://www.nature.com/articles/s42005-022-00987-z.pdf},
  poster   = {partial_observations_poster_presentation.pdf},
  code     = {https://github.com/peterparity/symder},
  selected = {true}
}

@inproceedings{alao2021discovering,
  title     = {Discovering Dynamical Parameters by Interpreting Echo State Networks},
  author    = {Oreoluwa Alao* and Peter Y. Lu* and Marin Soljačić},
  booktitle = {NeurIPS 2021 AI for Science Workshop},
  year      = {2021},
  url       = {https://openreview.net/forum?id=coaSxusdBLX},
  html      = {https://openreview.net/forum?id=coaSxusdBLX},
  pdf       = {https://openreview.net/pdf?id=coaSxusdBLX},
  poster    = {esn_manifold_learning_neurips2021_poster_v3.pdf},
  abstract  = {Reservoir computing architectures known as echo state networks (ESNs) have been shown to have exceptional predictive capabilities when trained on chaotic systems. However, ESN models are often seen as black-box predictors that lack interpretability. We show that the parameters governing the dynamics of a complex nonlinear system can be encoded in the learned readout layer of an ESN. We can extract these dynamical parameters by examining the geometry of the readout layer weights through principal component analysis. We demonstrate this approach by extracting the values of three dynamical parameters ($\sigma$, $\rho$, $\beta$) from a dataset of Lorenz systems where all three parameters are varying among different trajectories. Our proposed method not only demonstrates the interpretability of the ESN readout layer but also provides a computationally inexpensive, unsupervised data-driven approach for identifying uncontrolled variables affecting real-world data from nonlinear dynamical systems.},
  comment   = { --- <a href="https://ai4sciencecommunity.github.io/neurips21/award.html" target="_blank">Best Paper Award</a>},
  selected  = {true}
}

@article{9180100,
  author   = {Samuel {Kim} and Peter Y. {Lu} and Srijon {Mukherjee} and Michael {Gilbert} and Li {Jing} and Vladimir {Čeperić} and Marin {Soljačić}},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  title    = {Integration of Neural Network-Based Symbolic Regression in Deep Learning for Scientific Discovery},
  year     = {2021},
  volume   = {32},
  number   = {9},
  pages    = {4166-4177},
  doi      = {10.1109/TNNLS.2020.3017010},
  url      = {https://ieeexplore.ieee.org/document/9180100},
  html     = {https://ieeexplore.ieee.org/document/9180100},
  pdf      = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9180100},
  code     = {https://github.com/samuelkim314/DeepSymReg},
  abstract = {Symbolic regression is a powerful technique to discover analytic equations that describe data, which can lead to explainable models and the ability to predict unseen data. In contrast, neural networks have achieved amazing levels of accuracy on image recognition and natural language processing tasks, but they are often seen as black-box models that are difficult to interpret and typically extrapolate poorly. In this article, we use a neural network-based architecture for symbolic regression called the equation learner (EQL) network and integrate it with other deep learning architectures such that the whole system can be trained end-to-end through backpropagation. To demonstrate the power of such systems, we study their performance on several substantially different tasks. First, we show that the neural network can perform symbolic regression and learn the form of several functions. Next, we present an MNIST arithmetic task where a convolutional network extracts the digits. Finally, we demonstrate the prediction of dynamical systems where an unknown parameter is extracted through an encoder. We find that the EQL-based architecture can extrapolate quite well outside of the training data set compared with a standard neural network-based architecture, paving the way for deep learning to be applied in scientific exploration and discovery.},
  selected = {true}
}

@article{PhysRevX.10.031056,
  title     = {Extracting Interpretable Physical Parameters from Spatiotemporal Systems Using Unsupervised Learning},
  author    = {Lu, Peter Y. and Kim, Samuel and Soljačić, Marin},
  journal   = {Physical Review X},
  volume    = {10},
  issue     = {3},
  pages     = {031056},
  numpages  = {19},
  year      = {2020},
  month     = {Sep},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevX.10.031056},
  url       = {https://link.aps.org/doi/10.1103/PhysRevX.10.031056},
  html      = {https://link.aps.org/doi/10.1103/PhysRevX.10.031056},
  pdf       = {https://journals.aps.org/prx/pdf/10.1103/PhysRevX.10.031056},
  code      = {https://github.com/peterparity/PDE-VAE-pytorch},
  slides    = {pde_vae_slides_v3_long.pdf},
  video     = {https://www.youtube.com/watch?v=4tjD6_7Eclg},
  abstract  = {Experimental data are often affected by uncontrolled variables that make analysis and interpretation difficult. For spatiotemporal systems, this problem is further exacerbated by their intricate dynamics. Modern machine learning methods are particularly well suited for analyzing and modeling complex datasets, but to be effective in science, the result needs to be interpretable. We demonstrate an unsupervised learning technique for extracting interpretable physical parameters from noisy spatiotemporal data and for building a transferable model of the system. In particular, we implement a physics-informed architecture based on variational autoencoders that is designed for analyzing systems governed by partial differential equations. The architecture is trained end to end and extracts latent parameters that parametrize the dynamics of a learned predictive model for the system. To test our method, we train our model on simulated data from a variety of partial differential equations with varying dynamical parameters that act as uncontrolled variables. Numerical experiments show that our method can accurately identify relevant parameters and extract them from raw and even noisy spatiotemporal data (tested with roughly 10% added noise). These extracted parameters correlate well (linearly with <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><msup><mi>R</mi><mn>2</mn></msup><mo>&gt;</mo><mn>0.95</mn></math>) with the ground truth physical parameters used to generate the datasets. We then apply this method to nonlinear fiber propagation data, generated by an <i>ab initio</i> simulation, to demonstrate its capabilities on a more realistic dataset. Our method for discovering interpretable latent parameters in spatiotemporal systems will allow us to better analyze and understand real-world phenomena and datasets, which often have unknown and uncontrolled variables that alter the system dynamics and cause varying behaviors that are difficult to disentangle.},
  selected  = {true}
}

@article{Reichel:16,
  author    = {Kimberly S. Reichel and Peter Y. Lu and Sterling Backus and Rajind Mendis and Daniel M. Mittleman},
  journal   = {Optics Express},
  keywords  = {Waveguides; Spectroscopy, teraherz ; Wavelength filtering devices ; Extraordinary optical transmission; Light matter interactions; Subwavelength structures; Surface plasmons; Surface waves; Waveguide modes},
  number    = {25},
  pages     = {28221--28227},
  publisher = {OSA},
  title     = {Extraordinary optical transmission inside a waveguide: spatial mode dependence},
  volume    = {24},
  month     = {Dec},
  year      = {2016},
  url       = {http://www.opticsexpress.org/abstract.cfm?URI=oe-24-25-28221},
  html      = {https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-24-25-28221},
  pdf       = {https://www.osapublishing.org/oe/viewmedia.cfm?uri=oe-24-25-28221&seq=0},
  doi       = {10.1364/OE.24.028221},
  abstract  = {We study the influence of the input spatial mode on the extraordinary optical transmission (EOT) effect. By placing a metal screen with a 1D array of subwavelength holes inside a terahertz (THz) parallel-plate waveguide (PPWG), we can directly compare the transmission spectra with different input waveguide modes. We observe that the transmitted spectrum depends strongly on the input mode. A conventional description of EOT based on the excitation of surface plasmons is not predictive in all cases. Instead, we utilize a formalism based on impedance matching, which accurately predicts the spectral resonances for both TEM and non-TEM input modes.}
}

@article{PhysRevE.88.062204,
  title     = {Collision dynamics of particle clusters in a two-dimensional granular gas},
  author    = {Burton, Justin C. and Lu, Peter Y. and Nagel, Sidney R.},
  journal   = {Physical Review E},
  volume    = {88},
  issue     = {6},
  pages     = {062204},
  numpages  = {9},
  year      = {2013},
  month     = {Dec},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevE.88.062204},
  url       = {https://link.aps.org/doi/10.1103/PhysRevE.88.062204},
  html      = {https://link.aps.org/doi/10.1103/PhysRevE.88.062204},
  pdf       = {https://journals.aps.org/pre/pdf/10.1103/PhysRevE.88.062204},
  abstract  = {In a granular gas, inelastic collisions produce an instability in which the constituent particles cluster heterogeneously. These clusters then interact with each other, further decreasing their kinetic energy. We report experiments of the free collisions of dense clusters of particles in a two-dimensional geometry. The particles are composed of solid CO<span class="aps-inline-formula"><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><msub><mrow></mrow><mn>2</mn></msub></math></span>, which float nearly frictionlessly on a hot surface due to sublimated vapor. After two dense clusters of <span class="aps-inline-formula"><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mo>≈</mo></math></span>100 particles collide, there are two distinct stages of evolution. First, the translational kinetic energy rapidly decreases by over 90% as a “jamming front” sweeps across each cluster. Subsequently, the kinetic energy decreases more slowly as the particles approach the container boundaries. In this regime, the measured velocity distributions are non-Gaussian with long tails. Finally, we compare our experiments to computer simulations of colliding, two-dimensional, granular clusters composed of circular, viscoelastic particles with friction.}
}

@article{PhysRevLett.111.188001,
  title     = {Energy Loss at Propagating Jamming Fronts in Granular Gas Clusters},
  author    = {Burton, Justin C. and Lu, Peter Y. and Nagel, Sidney R.},
  journal   = {Physical Review Letters},
  volume    = {111},
  issue     = {18},
  pages     = {188001},
  numpages  = {4},
  year      = {2013},
  month     = {Oct},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.111.188001},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.111.188001},
  html      = {https://link.aps.org/doi/10.1103/PhysRevLett.111.188001},
  pdf       = {https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.111.188001},
  abstract  = {We explore the initial moments of impact between two dense granular clusters in a two-dimensional geometry. The particles are composed of solid <span class="aps-inline-formula"><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><msub><mi>CO</mi><mn>2</mn></msub></math></span> and are levitated on a hot surface. Upon collision, the propagation of a dynamic “jamming front” produces a distinct regime for energy dissipation in a granular gas in which the translational kinetic energy decreases by over 90%. Experiments and associated simulations show that the initial loss of kinetic energy obeys a power law in time <span class="aps-inline-formula"><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>Δ</mi><mi>E</mi><mo>=</mo><mo>−</mo><mi>K</mi><msup><mi>t</mi><mrow><mn>3</mn><mo>/</mo><mn>2</mn></mrow></msup></math></span>, a form that can be predicted from kinetic arguments.},
  selected  = {true}
}

